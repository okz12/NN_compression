{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "#Expand notebook to take full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "#Jupyter magic to notify when a cell finishes execution with %%notify command -- does not work with Jupyterlab\n",
    "import jupyternotify\n",
    "ip = get_ipython()\n",
    "ip.register_magics(jupyternotify.JupyterNotifyMagics)\n",
    "\n",
    "###\n",
    "import sys\n",
    "sys.path.insert(0,'../../src/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.nn.modules import Module\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "model_dir = \"./models/\"\n",
    "import model_archs\n",
    "from utils_plot import show_sws_weights, show_weights, print_dims, prune_plot, draw_sws_graphs, joint_plot, plot_data\n",
    "from utils_model import test_accuracy, train_epoch, retrain_sws_epoch, model_prune, get_weight_penalty, layer_accuracy\n",
    "from utils_misc import trueAfterN, logsumexp, root_dir, model_load_dir, get_ab, get_sparsity\n",
    "from utils_sws import GaussianMixturePrior, special_flatten, KL, compute_responsibilies, merger, sws_prune, sws_prune_l2, sws_prune_copy, sws_replace, compressed_model\n",
    "from mnist_loader import search_train_data, search_retrain_data, search_validation_data, train_data, test_data, batch_size\n",
    "from extract_targets import get_targets, get_layer_data\n",
    "from retrain_layer import retrain_layer\n",
    "retraining_epochs = 50\n",
    "\n",
    "\n",
    "test_data_full = Variable(test_data(fetch = \"data\")).cuda()\n",
    "test_labels_full = Variable(test_data(fetch = \"labels\")).cuda()\n",
    "#val_data_full = Variable(search_validation_data(fetch = \"data\")).cuda()\n",
    "#val_labels_full = Variable(search_validation_data(fetch = \"labels\")).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -141128.50\n",
      "Epoch: 50. Loss: -142871.94\n",
      "Original: 98.73% - Retrain: 97.68% - Prune: 96.91% - Quantize: 96.65% - Sparsity: 97.68%\n",
      "Original: 98.75% - Retrain: 97.97% - Prune: 97.01% - Quantize: 96.85% - Sparsity: 97.68%\n"
     ]
    }
   ],
   "source": [
    "layer_model_1, gmp_1, res_1 = retrain_layer(5000, 2, 5000, 10, 16, 1, 1e-4, 3, \"search\", \"SWSModel\", \"MSEHNA\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfile(mean, zmean, tau, temp, mixtures, model, data_size = \"full\",  scaling = False, model_save_dir = \"\", fn=\"\", file = \"res\"):\n",
    "    r = 30 if file == 'ures' else 50\n",
    "    exp_name = \"{}_m{}_zm{}_r{}_t{}_m{}_kdT{}_{}\".format(model, mean, zmean, r, tau, int(mixtures), int(temp),data_size) + fn\n",
    "    if (file=='res' or file == 'ures' or file == 'gmp'):\n",
    "        with open(model_save_dir + '/mnist_retrain_layer_{}_{}.p'.format(file, exp_name),'rb') as f:\n",
    "            file = pickle.load(f)\n",
    "    if (file=='model'):\n",
    "        file = torch.load(model_save_dir + '/mnist_retrain_layer_model_{}.m'.format(exp_name))\n",
    "    return file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp_1 = loadfile(mean = 5000, zmean = 5000, tau = 1e-6, temp = 1, mixtures = 15, model = \"LeNet_300_100FC1\", data_size = 'search', scaling = False, model_save_dir = \"./expfiles\", file = \"gmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Train all 3 layers\n",
    "#2. Unify model and replace priors\n",
    "#3. Optimize as usual\n",
    "def load_layer_retrain_LeNet_300_100(mean, zmean, temp, tau, mixtures, data_size, loss_type = 'MSEHNA', savedir = ''):\n",
    "    zvar = 10\n",
    "    var = 2\n",
    "    model_name = 'LeNet_300_100'\n",
    "    \n",
    "    #train 3 layers independently\n",
    "    #layer_model_1, gmp_1, res_1 = retrain_layer(mean, var, zmean, zvar, mixtures, 1, tau, 1, data_size, model_name, loss_type, savedir)\n",
    "    #layer_model_2, gmp_2, res_2 = retrain_layer(mean, var, zmean, zvar, mixtures, 1, tau, 2, data_size, model_name, loss_type, savedir)\n",
    "    #layer_model_3, gmp_3, res_3 = retrain_layer(mean, var, zmean, zvar, mixtures, temp, tau, 3, data_size, model_name, loss_type, savedir)\n",
    "    layer_model_1 = loadfile(mean = 5000, zmean = 5000, tau = 1e-7, temp = 1, mixtures = 15, model = \"LeNet_300_100FC1\", data_size = 'search', scaling = False, model_save_dir = \"./expfiles\", file = \"model\")\n",
    "    layer_model_2 = loadfile(mean = 5000, zmean = 5000, tau = 1e-5, temp = 1, mixtures = 15, model = \"LeNet_300_100FC2\", data_size = 'search', scaling = False, model_save_dir = \"./expfiles\", file = \"model\")\n",
    "    layer_model_3 = loadfile(mean = 5000, zmean = 5000, tau = 1e-4, temp = temp, mixtures = 15, model = \"LeNet_300_100FC3\", data_size = 'search', scaling = False, model_save_dir = \"./expfiles\", file = \"model\")\n",
    "    gmp_1 = loadfile(mean = 5000, zmean = 5000, tau = 1e-7, temp = 1, mixtures = 15, model = \"LeNet_300_100FC1\", data_size = 'search', scaling = False, model_save_dir = \"./expfiles\", file = \"gmp\")\n",
    "    gmp_2 = loadfile(mean = 5000, zmean = 5000, tau = 1e-5, temp = 1, mixtures = 15, model = \"LeNet_300_100FC2\", data_size = 'search', scaling = False, model_save_dir = \"./expfiles\", file = \"gmp\")\n",
    "    gmp_3 = loadfile(mean = 5000, zmean = 5000, tau = 1e-4, temp = temp, mixtures = 15, model = \"LeNet_300_100FC3\", data_size = 'search', scaling = False, model_save_dir = \"./expfiles\", file = \"gmp\")\n",
    "    \n",
    "    #transfer GMP to new unified model architecture\n",
    "    unified_model = sws_replace(model_archs.LeNet_300_100().cuda(), [layer_model_1.state_dict(), layer_model_2.state_dict(), layer_model_3.state_dict()])\n",
    "    gmp_u1 = GaussianMixturePrior(mixtures, [x for x in unified_model.fc1.parameters()], 0.99, zero_ab = get_ab(zmean, zvar), ab = get_ab(mean, var), means = gmp_1.means.clone().data.cpu().numpy(), scaling = False)\n",
    "    gmp_u2 = GaussianMixturePrior(mixtures, [x for x in unified_model.fc2.parameters()], 0.99, zero_ab = get_ab(zmean, zvar), ab = get_ab(mean, var), means = gmp_2.means.clone().data.cpu().numpy(), scaling = False)\n",
    "    gmp_u3 = GaussianMixturePrior(mixtures, [x for x in unified_model.fc3.parameters()], 0.99, zero_ab = get_ab(zmean, zvar), ab = get_ab(mean, var), means = gmp_3.means.clone().data.cpu().numpy(), scaling = False)\n",
    "\n",
    "    #load datasets\n",
    "    data_size = 'search'\n",
    "    if(data_size == 'search'):\n",
    "        train_dataset = search_train_data()\n",
    "        val_data_full = Variable(test_data(fetch='data')).cuda()\n",
    "        val_labels_full = Variable(test_data(fetch='labels')).cuda()\n",
    "        un_val_acc = test_accuracy(val_data_full, val_labels_full, unified_model)[0]\n",
    "    if(data_size == 'full'):\n",
    "        train_dataset = train_data()\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)    \n",
    "    \n",
    "    #unified accuracy\n",
    "    unify_res = plot_data(unified_model, mode = 'retrain', data_size = data_size, loss_type='CE', mv = (mean, var), zmv = (zmean, zvar), tau = tau, temp = temp, mixtures = mixtures)\n",
    "    unify_res.data_epoch(0, unified_model)\n",
    "    \n",
    "    #prune-stats before retraining\n",
    "    l1_prune = sws_prune_l2(dict((l, unified_model.state_dict()[l]) for l in (\"fc1.bias\", \"fc1.weight\")), gmp_u1)\n",
    "    l2_prune = sws_prune_l2(dict((l, unified_model.state_dict()[l]) for l in (\"fc2.bias\", \"fc2.weight\")), gmp_u2)\n",
    "    l3_prune = sws_prune_l2(dict((l, unified_model.state_dict()[l]) for l in (\"fc3.bias\", \"fc3.weight\")), gmp_u3)\n",
    "    pruned_model = sws_replace(unified_model, [l1_prune, l2_prune, l3_prune])\n",
    "    unify_res.data_prune(pruned_model)\n",
    "    preprune = copy.deepcopy(unify_res.prune_acc)\n",
    "    presparsity = unify_res.sparsity\n",
    "    \n",
    "    #init optimizers\n",
    "    loss_type = 'CESNT'\n",
    "    opt_1 = torch.optim.Adam([\n",
    "            {'params': unified_model.fc1.parameters(), 'lr': 1e-4},\n",
    "            {'params': [gmp_u1.means], 'lr': 3e-4},\n",
    "            {'params': [gmp_u1.gammas, gmp_u1.rhos], 'lr': 3e-3}])\n",
    "    opt_2 = torch.optim.Adam([\n",
    "            {'params': unified_model.fc2.parameters(), 'lr': 1e-4},\n",
    "            {'params': [gmp_u2.means], 'lr': 3e-4},\n",
    "            {'params': [gmp_u2.gammas, gmp_u2.rhos], 'lr': 3e-3}])\n",
    "    opt_3 = torch.optim.Adam([\n",
    "            {'params': unified_model.fc3.parameters(), 'lr': 1e-4},\n",
    "            {'params': [gmp_u3.means], 'lr': 3e-4},\n",
    "            {'params': [gmp_u3.gammas, gmp_u3.rhos], 'lr': 3e-3}])\n",
    "\n",
    "    #align and retrain\n",
    "    show_sws_weights(unified_model)\n",
    "    retraining_epochs=80\n",
    "    for epoch in range(retraining_epochs):\n",
    "        unified_model, loss = retrain_sws_epoch(unified_model, gmp_u1, opt_1, loader, 1e-7, 1, loss_type)\n",
    "        unified_model, loss = retrain_sws_epoch(unified_model, gmp_u2, opt_2, loader, 1e-5, 1, loss_type)\n",
    "        unified_model, loss = retrain_sws_epoch(unified_model, gmp_u3, opt_3, loader, 1e-4, 1, loss_type)\n",
    "        #test_acc = test_accuracy(test_data_full, test_labels_full, model_n)\n",
    "        #print('Epoch: {}. Test Accuracy: {:.2f}'.format(epoch+1, test_acc[0]))\n",
    "        #model_n, means = clamp_weights (model_n, means)\n",
    "        unify_res.data_epoch(epoch+1, unified_model)\n",
    "        \n",
    "        if (trueAfterN(epoch, 10)):\n",
    "            print (\"Epoch: {}, Test Acc: {:.2f}\".format(epoch+1, unify_res.test_accuracy[-1]))\n",
    "    show_sws_weights(unified_model)\n",
    "\n",
    "    #prune model\n",
    "    l1_prune = sws_prune_l2(dict((l, unified_model.state_dict()[l]) for l in (\"fc1.bias\", \"fc1.weight\")), gmp_u1)\n",
    "    l2_prune = sws_prune_l2(dict((l, unified_model.state_dict()[l]) for l in (\"fc2.bias\", \"fc2.weight\")), gmp_u2)\n",
    "    l3_prune = sws_prune_l2(dict((l, unified_model.state_dict()[l]) for l in (\"fc3.bias\", \"fc3.weight\")), gmp_u3)\n",
    "    pruned_model = sws_replace(unified_model, [l1_prune, l2_prune, l3_prune])\n",
    "    show_sws_weights(pruned_model)\n",
    "    unify_res.data_prune(pruned_model)\n",
    "    \n",
    "    ures = unify_res.gen_dict()\n",
    "    cm = compressed_model(pruned_model.state_dict(), [gmp_u1, gmp_u2, gmp_u3])\n",
    "    ures['cm'] = cm.get_cr_list()\n",
    "    ures['pre-sp'] = presparsity\n",
    "    ures['pre-prune_acc'] = preprune\n",
    "\n",
    "    show_sws_weights(pruned_model)\n",
    "        \n",
    "    ###SAVE MODEL, GMP, Res\n",
    "    if (savedir != \"\"):\n",
    "        #exp_name = \"{}_m{}_zm{}_r{}_t{}_m{}_kdT{}_{}\".format(unified_model.name, mean, zmean, retraining_epochs, tau, int(mixtures), int(temp), data_size)\n",
    "        exp_name = \"min_error3\"\n",
    "        torch.save(unified_model, savedir + '/mnist_retrain_layer_umodel_{}.m'.format(exp_name))\n",
    "        with open(savedir + '/mnist_retrain_layer_u1gmp_{}.p'.format(exp_name),'wb') as f:\n",
    "            pickle.dump(gmp_u1, f)\n",
    "        with open(savedir + '/mnist_retrain_layer_u2gmp_{}.p'.format(exp_name),'wb') as f:\n",
    "            pickle.dump(gmp_u2, f)\n",
    "        with open(savedir + '/mnist_retrain_layer_u3gmp_{}.p'.format(exp_name),'wb') as f:\n",
    "            pickle.dump(gmp_u3, f)\n",
    "        with open(savedir + '/mnist_retrain_layer_ures_{}.p'.format(exp_name),'wb') as f:\n",
    "            pickle.dump(ures, f)\n",
    "    \n",
    "    return layer_model_1, gmp_1, layer_model_2, gmp_2, layer_model_3, gmp_3, unified_model, ures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'model_archs.LeNet_300_100' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -2856.27\n",
      "Epoch: 50. Loss: -2856.26\n",
      "Original: 98.33% - Retrain: 97.99% - Prune: 91.80% - Quantize: 91.18% - Sparsity: 95.91%\n",
      "1e-06\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -1428.05\n",
      "Epoch: 50. Loss: -1428.02\n",
      "Original: 98.33% - Retrain: 98.35% - Prune: 96.67% - Quantize: 96.63% - Sparsity: 92.91%\n",
      "8e-07\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -1142.44\n",
      "Epoch: 50. Loss: -1142.48\n",
      "Original: 98.33% - Retrain: 98.34% - Prune: 96.98% - Quantize: 96.90% - Sparsity: 74.47%\n",
      "5e-07\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -713.99\n",
      "Epoch: 50. Loss: -713.99\n",
      "Original: 98.33% - Retrain: 98.48% - Prune: 97.58% - Quantize: 97.48% - Sparsity: 76.91%\n",
      "3e-07\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -428.38\n",
      "Epoch: 50. Loss: -428.36\n",
      "Original: 98.33% - Retrain: 98.49% - Prune: 97.84% - Quantize: 97.95% - Sparsity: 66.75%\n",
      "1e-07\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -142.77\n",
      "Epoch: 50. Loss: -142.77\n",
      "Original: 98.33% - Retrain: 98.45% - Prune: 98.17% - Quantize: 98.49% - Sparsity: 55.43%\n"
     ]
    }
   ],
   "source": [
    "for tau in [2e-6, 1e-6, 8e-7, 5e-7, 3e-7, 1e-7]:\n",
    "    print (tau)\n",
    "    layer_model_1, gmp_1, res_1 = retrain_layer(5000, 2, 5000, 10, 16, 1, tau, 1, \"full\", \"LeNet_300_100\", \"MSEHNA\", \"./files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'model_archs.LeNet_300_100' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25. Loss: -285650.97\n",
      "Epoch: 50. Loss: -285651.25\n",
      "Original: 98.33% - Retrain: 95.43% - Prune: 93.40% - Quantize: 93.23% - Sparsity: 98.21%\n",
      "0.0001\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -142823.59\n",
      "Epoch: 50. Loss: -142822.42\n",
      "Original: 98.33% - Retrain: 97.00% - Prune: 95.63% - Quantize: 95.51% - Sparsity: 96.67%\n",
      "8e-05\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -114258.34\n",
      "Epoch: 50. Loss: -114257.51\n",
      "Original: 98.33% - Retrain: 97.41% - Prune: 96.56% - Quantize: 96.38% - Sparsity: 95.87%\n",
      "5e-05\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -71410.68\n",
      "Epoch: 50. Loss: -71410.86\n",
      "Original: 98.33% - Retrain: 97.77% - Prune: 97.35% - Quantize: 97.36% - Sparsity: 93.90%\n",
      "3e-05\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -42846.04\n",
      "Epoch: 50. Loss: -42845.98\n",
      "Original: 98.33% - Retrain: 98.07% - Prune: 97.90% - Quantize: 97.81% - Sparsity: 91.16%\n",
      "1e-07\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -142.80\n",
      "Epoch: 50. Loss: -142.80\n",
      "Original: 98.33% - Retrain: 98.33% - Prune: 98.36% - Quantize: 98.37% - Sparsity: 18.03%\n"
     ]
    }
   ],
   "source": [
    "for tau in [2e-4, 1e-4, 8e-5, 5e-5, 3e-5, 1e-7]:\n",
    "    print (tau)\n",
    "    layer_model_2, gmp_2, res_2 = retrain_layer(5000, 2, 5000, 10, 16, 1, tau, 2, \"full\", \"LeNet_300_100\", \"MSEHNA\", \"./files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'model_archs.LeNet_300_100' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25. Loss: -14281.58\n",
      "Epoch: 50. Loss: -14281.57\n",
      "Original: 98.33% - Retrain: 98.27% - Prune: 98.28% - Quantize: 98.26% - Sparsity: 84.54%\n"
     ]
    }
   ],
   "source": [
    "for tau in [1e-5]:\n",
    "    print (tau)\n",
    "    layer_model_2, gmp_2, res_2 = retrain_layer(5000, 2, 5000, 10, 16, 1, tau, 2, \"full\", \"LeNet_300_100\", \"MSEHNA\", \"./files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'model_archs.LeNet_300_100' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25. Loss: -28565264.00\n",
      "Epoch: 50. Loss: -28565266.00\n",
      "Original: 98.33% - Retrain: 97.19% - Prune: 93.60% - Quantize: 93.79% - Sparsity: 78.51%\n",
      "0.01\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -14282633.00\n",
      "Epoch: 50. Loss: -14282633.00\n",
      "Original: 98.33% - Retrain: 98.02% - Prune: 97.46% - Quantize: 97.46% - Sparsity: 81.49%\n",
      "0.008\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -11426113.00\n",
      "Epoch: 50. Loss: -11426112.00\n",
      "Original: 98.33% - Retrain: 98.42% - Prune: 98.15% - Quantize: 98.13% - Sparsity: 73.07%\n",
      "0.005\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -7141316.50\n",
      "Epoch: 50. Loss: -7141315.50\n",
      "Original: 98.33% - Retrain: 98.32% - Prune: 98.05% - Quantize: 98.04% - Sparsity: 76.63%\n",
      "0.003\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -4284789.00\n",
      "Epoch: 50. Loss: -4284789.00\n",
      "Original: 98.33% - Retrain: 98.41% - Prune: 98.26% - Quantize: 98.15% - Sparsity: 79.90%\n",
      "0.002\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -2856524.75\n",
      "Epoch: 50. Loss: -2856524.25\n",
      "Original: 98.33% - Retrain: 98.39% - Prune: 98.22% - Quantize: 98.18% - Sparsity: 77.43%\n",
      "0.001\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -1428260.88\n",
      "Epoch: 50. Loss: -1428260.88\n",
      "Original: 98.33% - Retrain: 98.37% - Prune: 98.27% - Quantize: 98.20% - Sparsity: 75.45%\n"
     ]
    }
   ],
   "source": [
    "for tau in [2e-2, 1e-2, 8e-3, 5e-3, 3e-3, 1e-3]:\n",
    "    print (tau)\n",
    "    layer_model_3, gmp_3, res_3 = retrain_layer(5000, 2, 5000, 10, 16, 1, tau, 3, \"full\", \"LeNet_300_100\", \"MSEHNA\", \"./files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_model = sws_replace(model_archs.LeNet_300_100().cuda(), [layer_model_1.state_dict(), layer_model_2.state_dict(), layer_model_3.state_dict()])\n",
    "test_acc = test_accuracy(test_data_full, test_labels_full, unified_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'model_archs.LeNet_300_100' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -2844.18\n",
      "Epoch: 50. Loss: -2842.71\n",
      "Original: 98.33% - Retrain: 94.68% - Prune: 65.06% - Quantize: 69.47% - Sparsity: 98.54%\n",
      "1e-06\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -1416.57\n",
      "Epoch: 50. Loss: -1414.18\n",
      "Original: 98.33% - Retrain: 97.24% - Prune: 96.06% - Quantize: 96.00% - Sparsity: 88.06%\n",
      "8e-07\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -1129.57\n",
      "Epoch: 50. Loss: -1129.65\n",
      "Original: 98.33% - Retrain: 97.52% - Prune: 96.50% - Quantize: 95.34% - Sparsity: 78.47%\n",
      "5e-07\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -700.02\n",
      "Epoch: 50. Loss: -700.86\n",
      "Original: 98.33% - Retrain: 98.02% - Prune: 97.40% - Quantize: 97.44% - Sparsity: 81.12%\n",
      "3e-07\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -415.21\n",
      "Epoch: 50. Loss: -415.19\n",
      "Original: 98.33% - Retrain: 98.30% - Prune: 97.69% - Quantize: 97.64% - Sparsity: 72.50%\n",
      "1e-07\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -128.77\n",
      "Epoch: 50. Loss: -128.85\n",
      "Original: 98.33% - Retrain: 98.43% - Prune: 98.05% - Quantize: 98.06% - Sparsity: 64.84%\n",
      "0.0002\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -285638.53\n",
      "Epoch: 50. Loss: -285637.50\n",
      "Original: 98.33% - Retrain: 95.09% - Prune: 90.40% - Quantize: 90.73% - Sparsity: 98.06%\n",
      "0.0001\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -142810.30\n",
      "Epoch: 50. Loss: -142807.52\n",
      "Original: 98.33% - Retrain: 96.44% - Prune: 94.08% - Quantize: 93.89% - Sparsity: 97.62%\n",
      "8e-05\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -114244.00\n",
      "Epoch: 50. Loss: -114243.34\n",
      "Original: 98.33% - Retrain: 96.82% - Prune: 94.84% - Quantize: 94.73% - Sparsity: 97.47%\n",
      "5e-05\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -71396.64\n",
      "Epoch: 50. Loss: -71394.38\n",
      "Original: 98.33% - Retrain: 97.51% - Prune: 96.56% - Quantize: 96.48% - Sparsity: 96.18%\n",
      "3e-05\n",
      "0-component Mean: 5000.0 Variance: 10.0\n",
      "Non-zero component Mean: 5000.0 Variance: 2.0\n",
      "Epoch: 25. Loss: -42830.40\n"
     ]
    }
   ],
   "source": [
    "for tau in [2e-6, 1e-6, 8e-7, 5e-7, 3e-7, 1e-7]:\n",
    "    print (tau)\n",
    "    layer_model_1, gmp_1, res_1 = retrain_layer(5000, 2, 5000, 10, 16, 1, tau, 1, \"full\", \"LeNet_300_100\", \"MSEHA\", \"./mseha\")\n",
    "\n",
    "for tau in [2e-4, 1e-4, 8e-5, 5e-5, 3e-5, 1e-5]:\n",
    "    print (tau)\n",
    "    layer_model_2, gmp_2, res_2 = retrain_layer(5000, 2, 5000, 10, 16, 1, tau, 2, \"full\", \"LeNet_300_100\", \"MSEHA\", \"./mseha\")\n",
    "    \n",
    "for tau in [2e-2, 1e-2, 8e-3, 5e-3, 3e-3, 1e-3]:\n",
    "    print (tau)\n",
    "    layer_model_3, gmp_3, res_3 = retrain_layer(5000, 2, 5000, 10, 16, 1, tau, 3, \"full\", \"LeNet_300_100\", \"MSEHA\", \"./mseha\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
