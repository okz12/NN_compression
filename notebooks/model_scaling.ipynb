{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Expand notebook to take full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "#Jupyter magic to notify when a cell finishes execution with %%notify command -- does not work with Jupyterlab\n",
    "#import jupyternotify\n",
    "#ip = get_ipython()\n",
    "#ip.register_magics(jupyternotify.JupyterNotifyMagics)\n",
    "\n",
    "###\n",
    "import sys\n",
    "sys.path.insert(0,'../src/')\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800000 6056 54504 24224 512 161.53457849570924\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_recov = []\n",
    "for index, weight in zip(index_list, weight_list):\n",
    "    for z in range(int(index-1)):\n",
    "        R_recov.append(0)\n",
    "    R_recov.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(R_recov) - R).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f0e3e9dfda0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD4CAYAAAAjKGdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e3ea749b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from SALib.sample import saltelli\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "problem = {\n",
    "  'num_vars': 5,\n",
    "  'names': ['mean', 'var', 'tau', 'temp', 'mixtures'],\n",
    "  'bounds': [[np.log(0.01), np.log(1000)],\n",
    "    [np.log(0.001), np.log(100000)],\n",
    "    [np.log(1e-8), np.log(1e-3)],\n",
    "    [1.5,20.49],\n",
    "    [2.5,16.49]]\n",
    "}\n",
    "\n",
    "# Generate samples\n",
    "param_values = saltelli.sample(problem, 10000)\n",
    "\n",
    "params = {}\n",
    "params['mean'] = np.exp(param_values[:,0])\n",
    "params['var'] = np.exp(param_values[:,1])\n",
    "params['tau'] = np.exp(param_values[:,2])\n",
    "params['temp'] = np.around(param_values[:,3])\n",
    "params['mixtures'] = np.around(param_values[:,4])\n",
    "\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "    pickle.dump(\"sobol_search.p\", handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Softmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conv1': Variable containing:\n",
       " (  0  ,  0  ,.,.) = \n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -3.9411e-02  ...  -9.2864e-04  9.8376e-01  2.9793e-01\n",
       "  -4.8679e-02 -4.8679e-02 -3.9908e-01  ...  -8.6784e-02  1.4188e+00  2.0589e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.8679e-02 -4.3118e-02 -1.5655e-02  ...  -1.0699e-01 -5.0016e-02 -4.8679e-02\n",
       "  -8.1621e-02 -2.8955e-01 -9.0108e-02  ...  -1.0082e-01 -4.8679e-02 -4.8679e-02\n",
       "  -3.7796e-01 -1.0296e+00 -1.9022e-01  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       " \n",
       " (  0  ,  1  ,.,.) = \n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -1.5715e-02  ...  -1.5107e+00 -6.8004e-01  1.6714e-01\n",
       "  -4.6514e-03 -4.6514e-03 -3.0868e-01  ...   1.1335e+00  1.6749e-02 -3.7608e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.6514e-03 -1.1290e-02 -1.9535e-01  ...  -9.5631e-01 -6.8806e-03 -4.6514e-03\n",
       "  -9.7538e-02 -9.4411e-01 -7.9440e-01  ...  -9.1590e-02 -4.6514e-03 -4.6514e-03\n",
       "   3.9778e-01  1.4148e+00  1.1198e+00  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       " \n",
       " (  0  ,  2  ,.,.) = \n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -4.9928e-02  ...  -4.0915e-01 -5.8607e-01 -9.1433e-01\n",
       "  -2.8798e-02 -2.8798e-02 -1.9959e-01  ...   1.8232e-01  8.9796e-01  1.7833e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.8798e-02 -4.1476e-02 -7.1451e-02  ...   1.5009e-01 -2.9488e-02 -2.8798e-02\n",
       "  -1.2992e-01  2.1167e-01 -4.7630e-01  ...  -5.5706e-02 -2.8798e-02 -2.8798e-02\n",
       "  -3.5672e-01 -3.9065e-01  6.5827e-02  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "        ...  \n",
       " \n",
       " (  0  , 22  ,.,.) = \n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  2.6824e-02  ...   2.3810e-01 -2.9164e-01 -3.4315e-02\n",
       "   5.6456e-03  5.6456e-03  4.8014e-01  ...   1.2686e+00 -7.0985e-01 -1.1388e+00\n",
       "                  ...                   ⋱                   ...                \n",
       "   5.6456e-03  1.8353e-02  1.1739e-01  ...  -1.4423e+00  4.3692e-03  5.6456e-03\n",
       "   1.5547e-01  4.2509e-01  9.7139e-01  ...  -4.4135e-02  5.6456e-03  5.6456e-03\n",
       "   3.6622e-01  1.8821e+00  1.5413e+00  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       " \n",
       " (  0  , 23  ,.,.) = \n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  9.0497e-03  ...   1.0955e+00  1.8563e+00  6.2269e-01\n",
       "   2.0660e-02  2.0660e-02 -1.7878e-01  ...   1.0918e-02 -4.4788e-01 -4.5811e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "   2.0660e-02  1.3694e-02  5.7641e-02  ...  -5.1820e-01  2.0757e-02  2.0660e-02\n",
       "  -6.4440e-02  2.5193e-01  1.6065e+00  ...   2.4466e-02  2.0660e-02  2.0660e-02\n",
       "  -7.7064e-02 -4.0323e-02 -7.4776e-01  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       " \n",
       " (  0  , 24  ,.,.) = \n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02  1.1946e-02  ...   9.0130e-01  5.2243e-01  5.9210e-02\n",
       "  -2.4724e-02 -2.4724e-02  9.5130e-02  ...  -1.5437e+00  1.6113e-01  3.1059e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.4724e-02 -2.7223e-03  5.7057e-01  ...  -1.4475e-01 -2.4370e-02 -2.4724e-02\n",
       "   1.1803e-01  7.4960e-01  3.9172e-02  ...  -1.0885e-02 -2.4724e-02 -2.4724e-02\n",
       "  -4.5474e-01 -1.7910e+00 -1.3911e+00  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "          ⋮   \n",
       " \n",
       " (  1  ,  0  ,.,.) = \n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...  -1.2151e-01 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   9.6062e-01 -8.1986e-02 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   1.6906e+00  3.9354e-01 -4.8679e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.8679e-02 -5.1965e-01 -1.6423e+00  ...  -2.4730e-01 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -3.2777e-01 -1.5780e+00  ...  -8.6114e-02 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -5.9601e-02 -5.6041e-01  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       " \n",
       " (  1  ,  1  ,.,.) = \n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -1.1478e-01 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -6.2610e-02 -3.3770e-02 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -1.3503e+00  2.1883e-02 -4.6514e-03\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.6514e-03  2.3450e-01  1.4181e-01  ...  -7.5555e-01 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03  1.8980e-01  8.6833e-01  ...  -6.7069e-02 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03 -2.0280e-02 -6.7759e-02  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       " \n",
       " (  1  ,  2  ,.,.) = \n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -3.9077e-01 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -1.0863e+00 -1.4422e-01 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...   6.0345e-01 -1.1335e+00 -2.8798e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.8798e-02 -4.1730e-01 -3.7499e-01  ...  -8.7690e-02 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.6888e-01 -9.0743e-01  ...  -4.8117e-02 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -6.6948e-02 -5.3931e-01  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "        ...  \n",
       " \n",
       " (  1  , 22  ,.,.) = \n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -9.3889e-02  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -5.9634e-01 -1.6743e-02  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -1.8450e+00 -2.5539e-01  5.6456e-03\n",
       "                  ...                   ⋱                   ...                \n",
       "   5.6456e-03  4.1879e-01  2.6813e+00  ...  -8.4015e-01  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  2.0116e-01  2.1483e+00  ...  -3.0094e-02  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  5.4970e-03  4.1929e-01  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       " \n",
       " (  1  , 23  ,.,.) = \n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   1.3889e-01  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   1.1534e+00  4.3568e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   1.2930e+00  7.6415e-01  2.0660e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "   2.0660e-02 -1.3714e-01 -4.2999e-01  ...  -1.9546e-01  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02 -4.3519e-02 -8.7822e-01  ...   2.3393e-02  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  1.5337e-02 -3.8609e-01  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       " \n",
       " (  1  , 24  ,.,.) = \n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   3.5043e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   4.5097e-01 -1.0231e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   9.6419e-01  1.6301e-01 -2.4724e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.4724e-02 -3.8376e-01 -1.2106e+00  ...  -1.6645e-01 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.3638e-01 -1.7461e+00  ...  -1.4788e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.6859e-02 -2.0534e-01  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "          ⋮   \n",
       " \n",
       " (  2  ,  0  ,.,.) = \n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       "  -2.9525e-02  9.0837e-03 -3.9950e-02  ...  -1.2678e-01  6.2745e-01 -4.9707e-02\n",
       "  -2.4192e-01  1.7707e-01  5.3536e-01  ...  -1.9855e-02  1.1742e+00 -1.1818e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   3.6461e-01 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   6.2213e-01 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   2.7207e-01 -4.8679e-02 -4.8679e-02\n",
       " \n",
       " (  2  ,  1  ,.,.) = \n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       "  -2.7517e-02 -1.5266e-01 -1.0103e-01  ...  -8.2365e-01  4.4864e-03  4.8764e-03\n",
       "  -5.4340e-02 -7.6158e-01  3.6244e-02  ...  -5.4990e-01 -2.2340e-02 -4.6938e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -7.4198e-01 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -7.0676e-01 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -9.0546e-01 -4.6514e-03 -4.6514e-03\n",
       " \n",
       " (  2  ,  2  ,.,.) = \n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "  -7.2466e-02  5.3181e-02 -2.4392e-01  ...   4.4734e-01 -5.3318e-01 -1.7256e-01\n",
       "  -3.2616e-01  5.7508e-01 -7.7482e-01  ...   8.1311e-01 -3.9750e-01 -8.7474e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -9.9630e-01 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -8.9647e-01 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...   2.9941e-01 -2.8798e-02 -2.8798e-02\n",
       "        ...  \n",
       " \n",
       " (  2  , 22  ,.,.) = \n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       "   4.9415e-02 -5.2137e-02 -1.0173e-01  ...   1.3233e-01 -2.8663e-01  1.5948e-02\n",
       "   4.0720e-01  2.4303e-01 -3.4826e-01  ...   1.0831e+00 -9.8028e-01 -1.3457e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -1.5278e+00  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -1.6651e+00  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -1.6665e+00  5.6456e-03  5.6456e-03\n",
       " \n",
       " (  2  , 23  ,.,.) = \n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       "  -3.3342e-03  1.1872e-01  1.6722e-01  ...   4.5022e-01  1.0107e+00  5.5330e-02\n",
       "  -1.7392e-01  7.7297e-01  7.8198e-01  ...   6.9358e-01  6.3937e-01 -1.7676e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   2.6311e-01  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   3.6373e-01  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...  -5.3752e-01  2.0660e-02  2.0660e-02\n",
       " \n",
       " (  2  , 24  ,.,.) = \n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "   5.1061e-02  2.0750e-01  3.4336e-02  ...   4.4751e-01  2.6078e-01 -2.8169e-02\n",
       "   3.4057e-02  3.0505e-01  2.5388e-01  ...  -2.8209e-01  4.4525e-01 -3.4230e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   1.8569e-01 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   2.3800e-01 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   5.7900e-02 -2.4724e-02 -2.4724e-02\n",
       "  ...        \n",
       "          ⋮   \n",
       " \n",
       " (59997,  0  ,.,.) = \n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   7.0971e-02  6.5144e-01  8.2843e-01\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   4.8145e-01  9.2097e-01  7.5387e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.8679e-02 -4.7123e-02  2.9286e-01  ...  -2.1805e-01 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -9.2505e-01  5.6864e-01  ...  -1.1559e-01 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -8.5030e-01 -5.4588e-01  ...  -5.8707e-02 -4.8679e-02 -4.8679e-02\n",
       " \n",
       " (59997,  1  ,.,.) = \n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -1.5475e+00 -1.2577e+00  3.7941e-02\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...   1.6704e+00  6.7395e-01 -3.9118e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.6514e-03 -1.7191e-01 -4.7958e-01  ...  -4.2645e-01 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03  1.5451e-01 -5.6804e-01  ...  -1.4220e-01 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03  2.9922e-01  7.8932e-01  ...  -2.1370e-02 -4.6514e-03 -4.6514e-03\n",
       " \n",
       " (59997,  2  ,.,.) = \n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -3.3573e-01 -3.4573e-01 -9.7355e-01\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -8.1054e-02  6.8246e-01  5.8454e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.8798e-02  3.7374e-03 -1.5799e-01  ...  -1.6582e-01 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -1.6744e-01  7.2422e-01  ...  -5.0237e-02 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -4.1115e-01 -3.2650e-01  ...  -3.3973e-02 -2.8798e-02 -2.8798e-02\n",
       "        ...  \n",
       " \n",
       " (59997, 22  ,.,.) = \n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...   1.8484e-02 -1.0572e-01 -1.7758e-01\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...   1.6207e+00  2.8464e-01 -1.4031e+00\n",
       "                  ...                   ⋱                   ...                \n",
       "   5.6456e-03  6.9744e-02 -3.1192e-01  ...  -6.4183e-01  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  1.1285e+00  6.8831e-01  ...  -1.6368e-01  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  7.6068e-01  1.1169e+00  ...  -3.9275e-03  5.6456e-03  5.6456e-03\n",
       " \n",
       " (59997, 23  ,.,.) = \n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   1.1514e+00  1.8319e+00  1.3259e+00\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   5.7509e-01 -4.9189e-01 -7.2197e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "   2.0660e-02  1.1369e-01  5.2105e-01  ...  -1.6851e-01  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  8.0950e-04  1.1212e+00  ...  -2.5827e-02  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02 -4.8978e-01 -9.6556e-01  ...   2.1392e-02  2.0660e-02  2.0660e-02\n",
       " \n",
       " (59997, 24  ,.,.) = \n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   1.1429e+00  7.4863e-01  2.2486e-01\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...  -1.5072e+00 -5.5172e-01  2.5140e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.4724e-02  5.2930e-01  4.0197e-01  ...  -5.4557e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -1.0711e+00  4.6450e-01  ...  -2.4441e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -4.9476e-01 -1.4345e+00  ...  -2.2063e-02 -2.4724e-02 -2.4724e-02\n",
       "          ⋮   \n",
       " \n",
       " (59998,  0  ,.,.) = \n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   1.4170e+00  4.4515e-01 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   1.1224e+00 -3.3367e-01 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...  -2.5624e-01 -5.4696e-02 -4.8679e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.8679e-02 -1.9697e-01 -1.6132e+00  ...  -2.3217e-01 -8.6164e-02 -4.8679e-02\n",
       "  -4.8679e-02 -1.4665e-01 -9.1696e-01  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       " \n",
       " (59998,  1  ,.,.) = \n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...   8.6133e-02  3.2062e-02 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -6.1049e-01 -5.3923e-01 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -8.4980e-01 -1.4683e-02 -4.6514e-03\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.6514e-03  2.0289e-02  7.8640e-02  ...  -6.6147e-01 -9.7519e-02 -4.6514e-03\n",
       "  -4.6514e-03  5.7430e-02  3.2820e-01  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       " \n",
       " (59998,  2  ,.,.) = \n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -3.9884e-01 -2.8433e-01 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -5.7724e-02 -7.5125e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -7.4879e-02 -3.1903e-02 -2.8798e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.8798e-02 -1.8907e-01 -2.4303e-01  ...  -1.2070e-01 -4.9296e-02 -2.8798e-02\n",
       "  -2.8798e-02 -9.4683e-02 -3.9824e-01  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "        ...  \n",
       " \n",
       " (59998, 22  ,.,.) = \n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -5.0735e-02 -2.8844e-01  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -1.4242e+00 -7.1252e-01  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -1.0227e+00 -9.8277e-05  5.6456e-03\n",
       "                  ...                   ⋱                   ...                \n",
       "   5.6456e-03  1.1919e-01  2.1821e+00  ...  -7.4362e-01 -5.1968e-02  5.6456e-03\n",
       "   5.6456e-03  6.4978e-02  1.0116e+00  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       " \n",
       " (59998, 23  ,.,.) = \n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   1.8622e+00  8.0692e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...  -7.8960e-01 -2.1155e-01  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...  -2.7252e-01  2.1099e-02  2.0660e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "   2.0660e-02 -3.3892e-02 -4.6808e-01  ...  -2.3164e-01  2.1500e-02  2.0660e-02\n",
       "   2.0660e-02  1.7036e-03 -7.5319e-01  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       " \n",
       " (59998, 24  ,.,.) = \n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   2.8129e-01  9.2053e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   3.2542e-01 -9.5202e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...  -1.8790e-01 -2.3128e-02 -2.4724e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.4724e-02 -1.3774e-01 -1.6295e+00  ...  -1.7002e-01 -2.5498e-02 -2.4724e-02\n",
       "  -2.4724e-02 -8.3854e-02 -6.0450e-01  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "          ⋮   \n",
       " \n",
       " (59999,  0  ,.,.) = \n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...  -6.6496e-02 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -4.8679e-02 -4.8679e-02  ...   9.7191e-01 -1.2061e-01  1.2050e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.8679e-02 -3.0966e-01 -1.2828e+00  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -5.8311e-01 -1.3763e+00  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       "  -4.8679e-02 -1.4665e-01 -1.2412e+00  ...  -4.8679e-02 -4.8679e-02 -4.8679e-02\n",
       " \n",
       " (59999,  1  ,.,.) = \n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -4.3896e-02 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03 -4.6514e-03 -4.6514e-03  ...  -4.1734e-01 -1.9297e-01 -4.0532e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -4.6514e-03 -1.0482e-02  1.1766e-01  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03  4.3258e-01  1.0407e-01  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       "  -4.6514e-03  1.2383e-03  7.6209e-01  ...  -4.6514e-03 -4.6514e-03 -4.6514e-03\n",
       " \n",
       " (59999,  2  ,.,.) = \n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -1.4638e-01 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -2.8798e-02 -2.8798e-02  ...  -9.6330e-01 -6.9866e-01  1.4615e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.8798e-02 -2.4454e-01  2.0639e-02  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -4.1961e-01 -2.4468e-01  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "  -2.8798e-02 -1.7145e-01 -8.2246e-01  ...  -2.8798e-02 -2.8798e-02 -2.8798e-02\n",
       "        ...  \n",
       " \n",
       " (59999, 22  ,.,.) = \n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -3.2101e-02  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  5.6456e-03  5.6456e-03  ...  -4.7415e-01  1.6172e-02 -1.6163e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "   5.6456e-03  3.3901e-01  1.8662e+00  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  4.4384e-01  2.3958e+00  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       "   5.6456e-03  4.8126e-02  1.5240e+00  ...   5.6456e-03  5.6456e-03  5.6456e-03\n",
       " \n",
       " (59999, 23  ,.,.) = \n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   6.7632e-02  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02  2.0660e-02  2.0660e-02  ...   1.3223e+00  1.1257e-01  2.9460e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "   2.0660e-02 -1.3531e-01  4.5662e-01  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02 -1.1190e-01 -5.9403e-01  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       "   2.0660e-02 -6.2954e-03 -8.0092e-01  ...   2.0660e-02  2.0660e-02  2.0660e-02\n",
       " \n",
       " (59999, 24  ,.,.) = \n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...  -2.6668e-03 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -2.4724e-02 -2.4724e-02  ...   6.4970e-01  2.7772e-01  5.5185e-01\n",
       "                  ...                   ⋱                   ...                \n",
       "  -2.4724e-02 -1.4176e-01 -8.4605e-01  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -4.9262e-01 -1.2308e+00  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       "  -2.4724e-02 -7.2748e-02 -1.2214e+00  ...  -2.4724e-02 -2.4724e-02 -2.4724e-02\n",
       " [torch.cuda.FloatTensor of size 60000x25x12x12 (GPU 0)],\n",
       " 'conv2': Variable containing:\n",
       " (  0  ,  0  ,.,.) = \n",
       "  -8.1130e-01 -3.5802e+00 -8.7758e+00 -8.5437e+00 -1.6527e+00\n",
       "  -2.8275e+00  2.3161e+00 -5.9241e+00 -6.1116e-01 -6.6328e+00\n",
       "  -1.2177e+00 -8.1728e+00  1.2804e+01 -1.0335e+01 -4.8441e+00\n",
       "  -1.0245e+00 -1.1499e+00 -1.2974e+01  4.3599e+00 -7.8093e+00\n",
       "  -1.8588e+00 -7.9228e+00 -8.1642e+00 -4.8977e+00 -6.0128e-01\n",
       " \n",
       " (  0  ,  1  ,.,.) = \n",
       "  -1.3976e+00 -3.4546e+00 -1.7048e+00  9.7651e-01 -9.5030e-01\n",
       "  -1.0816e+00 -2.5479e+00 -5.4141e+00 -9.9044e+00 -1.1540e+01\n",
       "  -1.7500e+00 -4.2291e+00 -8.8079e+00 -2.2277e+00  8.5077e-01\n",
       "  -1.1684e+00 -1.6858e+00 -6.3057e+00 -5.8671e+00 -7.5828e-01\n",
       "  -2.7983e+00 -1.7835e+00  4.4136e+00 -1.1690e+01 -7.3801e+00\n",
       " \n",
       " (  0  ,  2  ,.,.) = \n",
       "  -4.2499e-01 -8.0759e+00 -1.5754e+01 -1.4860e+01 -8.2983e+00\n",
       "  -8.0987e-01  2.9306e+00  2.1777e+00  4.0594e+00  2.8448e-01\n",
       "   1.8505e-02 -1.1804e+00 -4.4323e+00 -8.5801e+00 -3.8481e+00\n",
       "  -1.4317e-01 -1.6460e-01 -3.9428e+00 -1.5684e+01 -1.2336e+00\n",
       "  -4.8331e+00 -1.8049e+01 -2.2763e+00  7.2781e+00 -7.3279e-02\n",
       "        ...  \n",
       " \n",
       " (  0  , 47  ,.,.) = \n",
       "  -4.1758e-01  1.3574e+00  3.3128e+00  3.4053e+00  7.8060e-01\n",
       "  -1.7739e+00 -2.1755e+01 -5.6432e+00 -1.4705e+01 -9.5984e+00\n",
       "   3.6210e-01  4.4518e-01 -1.9214e+01  1.1449e+01  3.2744e+00\n",
       "  -2.1940e-01  3.2081e-01  6.7128e-01 -2.1419e+01  5.4088e+00\n",
       "   1.3786e-01 -1.8638e+00 -7.9238e+00 -1.1912e+01 -9.1485e+00\n",
       " \n",
       " (  0  , 48  ,.,.) = \n",
       "  -2.2247e+00 -1.3776e+00 -5.7159e+00 -4.8208e+00 -2.1892e+00\n",
       "  -4.0331e+00 -1.4483e+01 -5.7241e+00 -1.2737e+01 -9.7956e+00\n",
       "  -1.6486e+00 -4.0897e+00 -3.8855e+00  1.6333e+00 -5.7464e+00\n",
       "  -2.0374e+00 -1.9706e+00 -6.8033e+00 -5.8245e+00 -6.1097e-01\n",
       "  -1.7714e+00 -5.4903e+00 -9.8524e+00 -1.4628e+01 -4.2990e+00\n",
       " \n",
       " (  0  , 49  ,.,.) = \n",
       "  -1.3887e-01  5.8533e+00  9.5519e+00  4.8251e+00 -5.4078e+00\n",
       "  -1.5022e+00 -1.5073e+01 -2.0641e+01 -1.6168e+01 -1.6633e+00\n",
       "   1.6047e-01  2.2913e+00 -5.5738e+00  1.4782e+00  2.0785e+00\n",
       "  -4.3633e-01  3.9645e-01  8.3760e+00 -3.4863e+00 -1.0949e+01\n",
       "   4.1421e+00  5.9940e+00 -1.6930e+01 -1.7197e+01 -5.5731e+00\n",
       "          ⋮   \n",
       " \n",
       " (  1  ,  0  ,.,.) = \n",
       "  -1.0245e+00 -9.3483e-01 -3.1466e+00 -3.2669e+00 -1.0099e+01\n",
       "  -1.0674e+00 -2.9488e+00 -1.1120e+01  1.3304e+00  6.0100e+00\n",
       "  -3.0404e-01 -1.1570e+01 -4.4688e+00 -5.5865e+00  2.3832e+00\n",
       "  -2.2376e+00 -5.5464e+00 -3.1084e+00 -7.7497e+00 -8.3458e+00\n",
       "  -4.2734e+00  9.9404e+00 -5.5673e+00 -6.5892e+00 -2.7669e+00\n",
       " \n",
       " (  1  ,  1  ,.,.) = \n",
       "  -1.1684e+00 -1.2459e+00 -4.2055e+00  5.0620e+00  1.1262e+00\n",
       "  -1.2581e+00 -4.4361e+00  4.5964e+00 -1.1536e+01 -5.7222e+00\n",
       "  -2.4050e+00  3.3276e+00 -1.3908e+01 -6.4049e+00 -4.4439e+00\n",
       "  -2.1470e+00 -7.4016e+00 -5.7805e-01 -3.5866e+00 -4.5764e+00\n",
       "  -3.4026e+00 -8.9835e+00  1.0277e+00 -6.7511e+00 -6.7304e+00\n",
       " \n",
       " (  1  ,  2  ,.,.) = \n",
       "  -1.4317e-01 -3.6626e-01 -8.7042e+00 -9.3791e+00 -5.5301e+00\n",
       "  -2.0401e-01 -8.7404e+00 -1.0531e+00  2.5967e+00 -3.4844e+00\n",
       "  -2.7688e+00 -1.1937e+00  9.2154e-03 -5.3875e+00 -7.3917e+00\n",
       "  -3.4448e+00 -4.9606e+00 -3.1048e+00 -1.3330e+01  1.0174e+00\n",
       "  -3.7766e+00 -6.7283e+00 -6.1094e+00  8.3177e+00 -2.6223e+00\n",
       "        ...  \n",
       " \n",
       " (  1  , 47  ,.,.) = \n",
       "  -2.1940e-01 -1.6303e-01 -2.4121e+00  2.2754e-01  1.0709e+01\n",
       "  -9.3575e-02 -4.3513e+00 -7.7801e+00 -1.4969e+01 -4.2743e+00\n",
       "  -3.5069e+00 -1.0191e+00 -9.4829e+00 -4.7865e+00 -1.0960e+01\n",
       "  -5.6173e+00 -5.0171e+00  1.4735e+00 -1.4252e+00 -1.2474e+01\n",
       "  -7.2256e+00 -1.1931e+01 -2.5995e+00 -6.9265e+00 -6.3672e+00\n",
       " \n",
       " (  1  , 48  ,.,.) = \n",
       "  -2.0374e+00 -2.2614e+00 -3.2399e+00 -2.4080e+00 -7.8526e-01\n",
       "  -2.1905e+00 -2.9102e+00 -1.0868e+01 -9.6078e+00  4.6755e+00\n",
       "  -3.7517e+00 -8.0916e+00 -1.0041e+01 -8.5241e+00 -3.5009e+00\n",
       "  -5.8714e+00 -8.2653e-02 -6.1977e+00 -3.7673e+00 -1.1090e+01\n",
       "  -7.0189e+00  5.9987e+00 -6.6096e+00 -1.1632e+01 -5.6531e+00\n",
       " \n",
       " (  1  , 49  ,.,.) = \n",
       "  -4.3633e-01  5.9325e-03  3.4510e+00 -5.5728e+00 -1.8809e+00\n",
       "  -4.3622e-01  1.2058e+00 -1.2711e+01 -1.8542e+01 -1.2249e+01\n",
       "   3.4059e-01 -7.9096e+00 -7.3657e+00  3.6094e+00 -8.0487e+00\n",
       "  -2.3123e+00 -3.5661e+00  2.5544e+00  5.3247e+00 -6.6198e+00\n",
       "  -1.9021e+00 -1.4117e+01 -1.4494e+01 -1.6301e+01 -3.6124e+00\n",
       "          ⋮   \n",
       " \n",
       " (  2  ,  0  ,.,.) = \n",
       "  -3.0606e+00 -2.0407e+00 -1.0245e+00 -7.5020e-01 -1.6898e+00\n",
       "  -1.5694e+00 -3.1852e+00 -1.0245e+00 -1.2774e+00 -6.8501e+00\n",
       "  -1.3842e+00 -6.7391e+00 -6.2009e+00 -5.7087e+00 -6.1479e+00\n",
       "   3.1634e+00 -1.2592e+00 -4.2463e+00 -1.4784e+00 -5.9468e+00\n",
       "  -2.2986e+00 -2.4557e+00 -1.2041e+00  6.8688e-01 -8.2472e+00\n",
       " \n",
       " (  2  ,  1  ,.,.) = \n",
       "  -1.7473e+00 -5.6150e-01 -1.1684e+00 -1.5514e+00 -1.6329e+00\n",
       "  -1.4415e+00 -1.5269e+00 -1.1684e+00 -2.3969e+00 -1.6919e+00\n",
       "  -5.7641e+00 -2.0107e+00 -1.8447e+00 -1.3367e+00 -5.2556e+00\n",
       "  -4.7838e+00 -2.6847e+00 -8.0366e+00 -1.0304e+01 -6.1193e+00\n",
       "  -2.4784e+00 -1.6740e+00 -8.9359e-01 -6.7955e+00 -3.8272e+00\n",
       " \n",
       " (  2  ,  2  ,.,.) = \n",
       "  -4.3271e+00 -1.0885e+00 -1.4317e-01 -6.0358e-01 -6.3240e+00\n",
       "  -4.0726e+00 -4.3323e-01 -1.4317e-01 -2.6392e+00 -3.2817e+00\n",
       "  -4.9769e+00 -1.0788e+01 -1.2812e+01 -9.4244e+00 -2.6573e+00\n",
       "   1.0448e+00  6.1264e+00  3.5119e+00 -7.1017e+00 -1.7057e+00\n",
       "  -8.6439e-01 -1.7650e+00 -7.7506e-01 -4.9712e+00 -1.8820e+00\n",
       "        ...  \n",
       " \n",
       " (  2  , 47  ,.,.) = \n",
       "   2.3777e+00  7.8762e-01 -2.1940e-01 -1.2163e+00  2.1463e+00\n",
       "  -1.8497e+00  2.6135e+00 -2.1940e-01 -3.7027e+00 -3.5079e+00\n",
       "  -5.0024e+00  3.7137e+00  2.9407e-01 -6.8206e-01 -7.2370e+00\n",
       "  -2.0506e+01 -1.2679e+01 -1.0712e+01 -1.0146e+01 -8.5104e+00\n",
       "   2.1865e-01 -2.7679e-01 -4.7924e-02 -9.3757e+00 -2.2843e+00\n",
       " \n",
       " (  2  , 48  ,.,.) = \n",
       "  -9.9349e-01 -3.1659e+00 -2.0374e+00 -2.6430e+00  6.2495e-01\n",
       "  -7.2585e-01 -3.9903e+00 -2.0374e+00 -4.6311e+00 -3.9878e+00\n",
       "   2.3201e+00 -5.9101e+00 -5.5765e+00 -3.5332e+00 -2.9706e+00\n",
       "  -1.2651e+01 -1.1496e+01 -1.2095e+01 -4.5869e+00 -2.2712e+00\n",
       "  -1.8766e+00 -3.0925e+00 -2.6758e+00 -5.2417e+00 -1.0443e+00\n",
       " \n",
       " (  2  , 49  ,.,.) = \n",
       "   2.0881e+00  1.0213e-01 -4.3633e-01 -4.5803e-01 -8.6891e-01\n",
       "  -7.6047e+00 -3.3551e+00 -4.3633e-01 -2.4437e-01 -6.9671e+00\n",
       "  -4.4592e+00  5.5192e+00  7.2269e+00 -2.9115e+00 -7.9749e+00\n",
       "  -8.4942e+00 -1.5576e+01 -1.0527e+01 -3.6808e+00 -7.8915e+00\n",
       "   1.5789e+00  1.3911e+00 -8.1896e-02 -2.0278e+00 -8.1380e+00\n",
       "  ...        \n",
       "          ⋮   \n",
       " \n",
       " (59997,  0  ,.,.) = \n",
       "  -1.0245e+00 -1.1562e+00 -2.4424e+00 -5.9586e+00 -2.1301e+00\n",
       "  -9.5973e-01  2.7914e+00 -1.1267e+01 -7.5424e+00 -6.9576e+00\n",
       "  -1.3504e+00 -5.7272e+00  1.1749e+01 -9.9929e+00 -3.0606e+00\n",
       "  -1.0245e+00 -4.3693e+00 -9.7837e+00  1.0092e+01 -6.4616e+00\n",
       "   9.2355e-01 -8.2535e+00 -7.3081e+00 -8.8431e+00 -2.9593e+00\n",
       " \n",
       " (59997,  1  ,.,.) = \n",
       "  -1.1684e+00 -1.2294e+00 -1.7840e+00 -2.8064e+00 -9.0106e-01\n",
       "  -1.2392e+00 -4.3991e+00  5.2083e+00 -6.3158e+00 -1.3812e+01\n",
       "  -1.0647e+00 -5.1176e+00 -1.3671e+01 -7.3029e-01  1.1777e-02\n",
       "  -1.1684e+00 -4.3634e+00 -6.0163e+00 -8.6466e+00 -1.2923e+00\n",
       "  -3.2998e+00 -6.6275e-01 -2.7652e+00 -1.1631e+00 -4.1909e+00\n",
       " \n",
       " (59997,  2  ,.,.) = \n",
       "  -1.4317e-01 -3.0516e-01 -3.5139e+00 -1.3735e+01 -1.1958e+01\n",
       "  -2.6113e-01 -8.3745e+00 -2.4418e+00  6.2444e+00  4.2027e-01\n",
       "   5.1519e-02 -6.9888e-01 -3.0851e+00 -7.7877e+00 -1.7025e+00\n",
       "  -1.4317e-01  2.9346e-01 -1.7110e+00 -7.6771e+00 -1.4011e+00\n",
       "  -5.3755e+00 -8.0920e+00 -1.0566e+01 -7.6149e+00  5.4568e-01\n",
       "        ...  \n",
       " \n",
       " (59997, 47  ,.,.) = \n",
       "  -2.1940e-01  1.2376e-02  1.3945e+00 -5.3875e-01 -2.6855e+00\n",
       "  -5.6879e-01 -6.8162e+00  4.5141e-02 -9.0588e+00 -1.0560e+01\n",
       "  -2.0844e-02 -1.0316e+01 -1.6210e+01  1.0678e+01  1.9051e+00\n",
       "  -2.1940e-01  2.9545e+00 -7.5067e+00 -1.7324e+01  4.5293e+00\n",
       "  -4.8511e-02  6.2966e+00  2.5830e+00 -7.5924e+00 -1.4951e+00\n",
       " \n",
       " (59997, 48  ,.,.) = \n",
       "  -2.0374e+00 -2.2682e+00 -2.3426e+00 -5.1371e+00 -4.8148e+00\n",
       "  -2.1657e+00 -3.4548e+00 -5.8856e+00 -1.4501e+01 -9.7582e+00\n",
       "  -2.1925e+00 -1.0739e+01  3.2105e+00 -5.3445e-01 -4.5834e+00\n",
       "  -2.0374e+00 -8.0289e-01 -1.0262e+01  2.2948e+00 -3.6245e+00\n",
       "  -4.2609e-01 -2.5713e+00 -4.1767e+00 -1.0359e+01 -3.0979e+00\n",
       " \n",
       " (59997, 49  ,.,.) = \n",
       "  -4.3633e-01  7.0835e-02  4.6177e+00  8.3084e+00 -2.5360e+00\n",
       "  -3.5703e-01 -2.0154e+00 -1.4469e+01 -1.5389e+01 -2.9281e+00\n",
       "  -4.3906e-01 -3.1626e+00 -1.2003e+01  1.2302e+00  5.5965e-01\n",
       "  -4.3633e-01  3.2117e+00  1.8533e+00 -1.0465e+01 -4.9310e+00\n",
       "   1.5574e+00  6.4914e+00  8.8054e+00 -8.5747e-01 -6.7820e+00\n",
       "          ⋮   \n",
       " \n",
       " (59998,  0  ,.,.) = \n",
       "  -1.0245e+00 -1.0245e+00 -1.0215e+00 -7.9897e+00 -3.2785e+00\n",
       "  -1.0245e+00 -3.9423e-01 -6.9720e+00 -6.8870e+00 -1.2264e+00\n",
       "  -8.0115e-01 -5.4994e+00 -7.3134e+00 -5.3344e+00 -3.2251e+00\n",
       "  -1.0034e+00 -8.1652e+00 -4.6777e+00 -4.9209e+00 -2.8921e+00\n",
       "  -4.7880e+00  8.8975e+00 -2.1488e+00 -2.1363e+00 -3.7657e+00\n",
       " \n",
       " (59998,  1  ,.,.) = \n",
       "  -1.1684e+00 -1.1684e+00 -1.9642e+00  2.5963e+00 -1.0076e+01\n",
       "  -1.1684e+00 -2.3751e+00  1.7164e+00 -1.2284e+01 -1.6149e+00\n",
       "  -1.4607e+00 -1.4572e+00 -1.2169e+01 -4.2829e+00 -1.6782e-01\n",
       "  -2.5141e+00 -4.3175e+00 -1.6170e+00 -4.8374e+00 -6.2773e+00\n",
       "  -2.2024e+00 -7.7795e+00 -1.4728e+00 -2.8494e+00 -6.9001e+00\n",
       " \n",
       " (59998,  2  ,.,.) = \n",
       "  -1.4317e-01 -1.4317e-01 -1.8816e+00 -7.0410e+00  5.5596e-01\n",
       "  -1.4317e-01 -1.9918e+00 -8.5393e+00  8.6069e-01 -1.7551e+00\n",
       "  -4.1504e-01 -8.8330e+00 -6.4958e-01 -1.3724e+01 -1.0099e+01\n",
       "  -3.5345e+00  1.8212e-01 -3.8633e+00 -2.7425e-02 -1.4919e+00\n",
       "  -2.5384e+00 -9.4865e-01  5.2338e+00  5.5291e+00 -2.8362e+00\n",
       "        ...  \n",
       " \n",
       " (59998, 47  ,.,.) = \n",
       "  -2.1940e-01 -2.1940e-01 -8.4201e-01 -4.6552e+00 -1.0025e+01\n",
       "  -2.1940e-01 -2.0357e+00 -3.9078e+00 -7.9029e+00 -2.7526e+00\n",
       "  -7.8534e-01 -4.1922e+00 -9.9188e+00 -4.3771e+00  4.8632e+00\n",
       "  -4.7684e+00 -1.7352e+00 -6.2192e+00 -9.8236e+00 -1.2732e+01\n",
       "  -4.5874e+00 -1.6903e+01 -2.5427e+00 -7.2183e+00 -5.0513e+00\n",
       " \n",
       " (59998, 48  ,.,.) = \n",
       "  -2.0374e+00 -2.0374e+00 -2.3941e+00 -7.2455e+00 -7.2032e+00\n",
       "  -2.0374e+00 -3.1032e+00 -6.0389e+00 -8.0133e+00 -3.8949e+00\n",
       "  -2.5262e+00 -2.6821e+00 -8.1411e+00 -6.5818e+00  8.5135e-01\n",
       "  -4.6717e+00 -3.6281e+00 -8.1313e+00 -4.8025e+00 -1.0187e+01\n",
       "  -5.5280e+00 -2.7838e+00 -8.4086e+00 -9.8533e+00 -4.8896e+00\n",
       " \n",
       " (59998, 49  ,.,.) = \n",
       "  -4.3633e-01 -4.3633e-01  1.7172e+00 -4.9332e+00 -1.4360e+01\n",
       "  -4.3633e-01  1.2116e+00 -5.1423e+00 -7.3283e+00 -4.8034e-02\n",
       "  -2.5151e-01 -1.4690e+00 -4.5365e+00  6.9964e+00 -1.9479e+00\n",
       "  -1.1581e+00 -3.6128e+00 -1.9740e+00 -6.8253e+00 -1.0186e+01\n",
       "  -2.1219e+00 -1.5568e+01 -1.4152e+01 -7.5958e+00 -3.5053e-01\n",
       "          ⋮   \n",
       " \n",
       " (59999,  0  ,.,.) = \n",
       "  -1.0245e+00 -1.0657e+00 -1.6264e+00 -6.2862e+00 -6.8604e+00\n",
       "  -1.0245e+00  9.2394e-02 -6.7695e+00  1.2933e+00  3.0797e+00\n",
       "  -1.0245e+00 -8.5633e-01 -7.9681e+00 -7.0594e+00 -6.2070e+00\n",
       "  -6.3727e-01 -7.5072e+00 -8.2043e+00 -5.2701e+00 -2.1163e+00\n",
       "   9.0852e-01 -1.0307e+01 -1.7593e+00 -1.6337e+00 -1.0245e+00\n",
       " \n",
       " (59999,  1  ,.,.) = \n",
       "  -1.1684e+00 -1.1922e+00 -1.5780e+00 -1.5484e+00  1.1820e+00\n",
       "  -1.1684e+00 -2.5422e+00  2.2601e+00 -5.4763e+00 -4.8201e+00\n",
       "  -1.1684e+00 -1.8211e+00 -1.2399e+01  2.1073e-01 -6.0809e+00\n",
       "  -1.5006e+00 -4.9169e+00  6.7969e-01 -1.3718e+01 -1.8686e+00\n",
       "  -2.6318e+00 -6.3344e+00 -2.4519e+00 -6.7640e-01 -1.1684e+00\n",
       " \n",
       " (59999,  2  ,.,.) = \n",
       "  -1.4317e-01 -1.4844e-01 -2.3033e+00 -1.0589e+01 -6.2946e+00\n",
       "  -1.4317e-01 -2.8432e+00 -5.0229e+00  5.1651e+00 -7.5244e+00\n",
       "  -1.4317e-01 -3.1439e+00 -9.0014e+00 -1.1030e+01  7.5211e+00\n",
       "  -6.7902e-01 -1.1595e+01  3.9294e+00 -1.8568e+00 -2.6905e+00\n",
       "  -4.5086e+00 -1.1413e+00 -5.8783e+00 -1.0244e+00 -1.4317e-01\n",
       "        ...  \n",
       " \n",
       " (59999, 47  ,.,.) = \n",
       "  -2.1940e-01 -1.3885e-01  1.1375e+00  4.0678e+00  4.6356e+00\n",
       "  -2.1940e-01 -3.1525e+00 -5.9933e+00 -1.8837e+01 -1.4733e+00\n",
       "  -2.1940e-01 -5.1749e+00 -6.2291e+00 -5.5795e+00 -1.0261e+01\n",
       "  -8.3650e-01 -6.3744e+00 -6.2876e+00 -7.3121e+00 -2.8893e+00\n",
       "  -7.5534e+00  2.0771e+00 -6.7632e+00  4.9782e-01 -2.1940e-01\n",
       " \n",
       " (59999, 48  ,.,.) = \n",
       "  -2.0374e+00 -2.0883e+00 -1.4009e+00 -2.1645e+00 -5.2354e+00\n",
       "  -2.0374e+00 -3.4917e+00 -9.1409e+00 -1.5607e+01  4.3762e+00\n",
       "  -2.0374e+00 -4.2722e+00 -1.2725e+00 -9.4369e+00 -1.3208e+01\n",
       "  -2.4594e+00 -6.9774e+00 -4.4579e+00 -8.2251e+00 -4.2489e+00\n",
       "  -5.2379e+00  5.5311e-01 -2.6825e+00 -3.9684e+00 -2.0374e+00\n",
       " \n",
       " (59999, 49  ,.,.) = \n",
       "  -4.3633e-01 -4.4524e-01  2.7060e+00  8.1553e+00  4.4341e+00\n",
       "  -4.3633e-01  8.3635e-01 -9.7909e+00 -2.0381e+01 -1.1435e+01\n",
       "  -4.3633e-01 -2.8489e+00  1.9483e+00 -3.7253e+00 -1.1233e+01\n",
       "   4.1930e-01  2.9018e+00 -1.5056e+01 -2.6233e+00  1.2569e+00\n",
       "  -3.0035e+00 -2.3209e+00 -1.1221e+00 -7.5005e-01 -4.3633e-01\n",
       " [torch.cuda.FloatTensor of size 60000x50x5x5 (GPU 0)],\n",
       " 'fc1': \n",
       "  0.0000  0.0000  0.0003  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0001\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000  0.0002  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " [torch.cuda.FloatTensor of size 60000x500 (GPU 0)],\n",
       " 'fc2': \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  1.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  ...   0.0000  1.0000  0.0000\n",
       " [torch.cuda.FloatTensor of size 60000x10 (GPU 0)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.nn.modules import Module\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import model_archs\n",
    "from utils_plot import show_sws_weights, show_weights, print_dims, prune_plot, draw_sws_graphs, joint_plot\n",
    "from utils_model import test_accuracy, train_epoch, retrain_sws_epoch, model_prune, get_weight_penalty\n",
    "from utils_misc import trueAfterN, logsumexp, root_dir, model_load_dir, root_dir\n",
    "from utils_sws import GaussianMixturePrior, special_flatten, KL, compute_responsibilies, merger, sws_prune\n",
    "from mnist_loader import search_train_data, search_retrain_data, search_validation_data, train_data, test_data, batch_size\n",
    "from extract_targets import get_targets\n",
    "import copy\n",
    "import pickle\n",
    "import argparse\n",
    "retraining_epochs = 50\n",
    "\n",
    "get_targets('mnist_SWSModel_100_full', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_LeNet_300_100_100_full.m\t  mnist_SWSModel_100_full.m\r\n",
      "mnist_LeNet_300_100_100_search.m  mnist_SWSModel_100_search.m\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Softmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-fa797fd5f668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparsity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_prune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SWSModel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'search'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-142-fa797fd5f668>\u001b[0m in \u001b[0;36mretrain_model\u001b[0;34m(alpha, beta, tau, temp, mixtures, model_name, data_size, model_save_dir)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}{}_targets/{}.out.m\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_load_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"search\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fc2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "#execution example: python retrain.py --model SWSModel --alpha 2500 --beta 10 --tau 1e-6 --mixtures 8 --temp 10\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.nn.modules import Module\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import model_archs\n",
    "from utils_plot import show_sws_weights, show_weights, print_dims, prune_plot, draw_sws_graphs, joint_plot\n",
    "from utils_model import test_accuracy, train_epoch, retrain_sws_epoch, model_prune, get_weight_penalty\n",
    "from utils_misc import trueAfterN, logsumexp, root_dir, model_load_dir, root_dir\n",
    "from utils_sws import GaussianMixturePrior, special_flatten, KL, compute_responsibilies, merger, sws_prune\n",
    "from mnist_loader import search_train_data, search_retrain_data, search_validation_data, train_data, test_data, batch_size\n",
    "import copy\n",
    "import pickle\n",
    "import argparse\n",
    "retraining_epochs = 50\n",
    "\n",
    "###\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def retrain_model(alpha, beta, tau, temp, mixtures, model_name, data_size, model_save_dir = \"\"):\n",
    "    if(data_size == 'search'):\n",
    "        train_dataset = search_retrain_data\n",
    "        val_data_full = Variable(search_validation_data(fetch='data')).cuda()\n",
    "        val_labels_full = Variable(search_validation_data(fetch='labels')).cuda()\n",
    "        (x_start, x_end) = (40000, 50000)\n",
    "    if(data_size == 'full'):\n",
    "        train_dataset = train_data\n",
    "        (x_start, x_end) = (0, 60000)\n",
    "    test_data_full = Variable(test_data(fetch='data')).cuda()\n",
    "    test_labels_full = Variable(test_data(fetch='labels')).cuda()\n",
    "        \n",
    "    model_file = 'mnist_{}_{}_{}'.format(model_name, 100, data_size)\n",
    "    model = torch.load(model_load_dir + model_file + '.m').cuda()\n",
    "        \n",
    "    if temp == 0:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loader = torch.utils.data.DataLoader(dataset=train_dataset(), batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "        output = torch.load(\"{}{}_targets/{}.out.m\".format(model_load_dir, model_file.replace(\"search\", \"full\"), \"fc2\"))[x_start:x_end]\n",
    "        output = (nn.Softmax(dim=1)(output/temp)).data\n",
    "        dataset = torch.utils.data.TensorDataset(train_dataset(fetch='data'), output)\n",
    "        loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    exp_name = \"m{}_a{}_b{}_r{}_t{}_kdT{}_{}\".format(model.name, alpha, beta, retraining_epochs, tau, temp, data_size)\n",
    "    gmp = GaussianMixturePrior(mixtures, [x for x in model.parameters()], 0.99, ab = (alpha, beta), scaling=True)\n",
    "    gmp.print_batch = False\n",
    "    \n",
    "    scaling = True\n",
    "\n",
    "    optimizable_params = [\n",
    "        {'params': model.parameters(), 'lr': 1e-4},\n",
    "        {'params': [gmp.means], 'lr': 1e-4},\n",
    "        {'params': [gmp.gammas, gmp.rhos], 'lr': 3e-3}]\n",
    "    if (scaling):\n",
    "        optimizable_params = optimizable_params + [{'params': gmp.scale, 'lr': 1e-4}]\n",
    "        exp_name = exp_name + \"_scaling\"\n",
    "\n",
    "    opt = torch.optim.Adam(optimizable_params)\n",
    "\n",
    "    for epoch in range(retraining_epochs):\n",
    "        model, loss = retrain_sws_epoch(model, gmp, opt, criterion, loader, tau)\n",
    "        \n",
    "        print('Epoch: {}'.format(epoch+1))\n",
    "        if (trueAfterN(epoch, 5)):\n",
    "            test_acc = test_accuracy(test_data_full, test_labels_full, model)\n",
    "            print('Epoch: {}. Test Accuracy: {:.2f}'.format(epoch+1, test_acc[0]))\n",
    "            print(gmp.scale)\n",
    "            \n",
    "            ###\n",
    "            show_sws_weights(model = model, means = list(gmp.means.data.clone().cpu()), precisions = list(gmp.gammas.data.clone().cpu()), epoch = epoch)\n",
    "            print('Epoch: {}. Test Accuracy: {:.2f}'.format(epoch+1, test_acc[0]))\n",
    "    \n",
    "    if(model_save_dir!=\"\"):\n",
    "        torch.save(model, model_save_dir + '/mnist_retrain_{}.m'.format(exp_name))\n",
    "        with open(model_save_dir + '/mnist_retrain_{}_gmp.p'.format(exp_name),'wb') as f:\n",
    "            pickle.dump(gmp, f)\n",
    "    \n",
    "    test_accuracy_pre = float((test_accuracy(test_data_full, test_labels_full, model)[0]))\n",
    "    val_accuracy_pre = 0 if (data_size != 'search') else float((test_accuracy(val_data_full, val_labels_full, model)[0]))\n",
    "    \n",
    "    \n",
    "    model_prune = copy.deepcopy(model)\n",
    "    model_prune.load_state_dict(sws_prune(model_prune, gmp))\n",
    "    prune_acc = (test_accuracy(test_data_full, test_labels_full, model_prune))\n",
    "    test_accuracy_prune = float((test_accuracy(test_data_full, test_labels_full, model_prune)[0]))\n",
    "    val_accuracy = 0 if (data_size != 'search') else float((test_accuracy(val_data_full, val_labels_full, model_prune)[0]))\n",
    "    sparsity = (special_flatten(model_prune.state_dict())==0).sum()/(special_flatten(model_prune.state_dict())>0).numel() * 100\n",
    "    print('Retrain Test: {:.2f}, Retrain Validation: {:.2f}, Prune Test: {:.2f}, Prune Validation: {:.2f}, Prune Sparsity: {:.2f}'\n",
    "          .format(test_accuracy_pre, val_accuracy_pre, test_accuracy_prune, val_accuracy, sparsity))\n",
    "    \n",
    "        \n",
    "    return val_accuracy, sparsity, model, model_prune, gmp\n",
    "    \n",
    "_, _, norm, prune, gmp = retrain_model(25000, 10, 1e-6, 3, 4, 'SWSModel', 'search', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.9198\n",
       " 2.2443\n",
       " 1.3385\n",
       " 1.9122\n",
       "[torch.cuda.FloatTensor of size 4 (GPU 0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmp.scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa23afcf9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa23b2bbd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_sws_weights(prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.3679\n",
       "-0.1001\n",
       " 0.1602\n",
       "[torch.cuda.FloatTensor of size 3 (GPU 0)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmp_bkp = copy.deepcopy(gmp.means)\n",
    "gmp_bkp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned - Epoch: 1. Test Accuracy: 74.78\n",
      "Epoch: 1. Test Accuracy: 93.09\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-27343d07dd08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mcluster_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m#print (layer, float(mean), float(gmp.scale[int(l/2)].exp()), cluster_sum, cluster_elem)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mnew_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcluster_elem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mnew_means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "model = copy.deepcopy(prune)\n",
    "\n",
    "data_size ='search'\n",
    "\n",
    "if(data_size == 'search'):\n",
    "    train_dataset = search_train_data()\n",
    "if(data_size == 'full'):\n",
    "    train_dataset = train_data()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_full = Variable(test_data(fetch='data')).cuda()\n",
    "test_labels_full = Variable(test_data(fetch='labels')).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay = 0.000)\n",
    "for epoch in range(10):\n",
    "    idx_dict = copy.deepcopy(model.state_dict())\n",
    "    value_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    #keeping index of mixture, layer by layer\n",
    "    for l, layer in enumerate(model.state_dict()):\n",
    "        for m, mean in enumerate(gmp.means):\n",
    "            idx_dict[layer] [(model.state_dict()[layer] == mean / gmp.scale[int(l/2)].exp().data)] = m + 1 \n",
    "\n",
    "    test_acc = test_accuracy(test_data_full, test_labels_full, model)\n",
    "    print('Pruned - Epoch: {}. Test Accuracy: {:.2f}'.format(1, test_acc[0]))\n",
    "    #show_sws_weights(model)\n",
    "\n",
    "    ### Mark which tensors belong to which layer in matrices -- mixture_occ\n",
    "\n",
    "    model, loss = train_epoch(model, optimizer, criterion, train_loader)\n",
    "\n",
    "    ### Find the new means -- use mixture occ - for scaling use multipliers across all layers \n",
    "\n",
    "    test_acc = test_accuracy(test_data_full, test_labels_full, model)\n",
    "    print('Epoch: {}. Test Accuracy: {:.2f}'.format(1, test_acc[0]))\n",
    "    #show_sws_weights(model)\n",
    "    #\t\t-> Calculate new mean for each \"cluster\"\n",
    "    new_means = gmp.means.clone()\n",
    "    for m, mean in enumerate(new_means):\n",
    "        #print(m, mean)\n",
    "        cluster_sum = 0\n",
    "        cluster_elem = 0\n",
    "        for l, layer in enumerate(model.state_dict()):\n",
    "            cluster_elem += (idx_dict[layer] == m+1).sum()\n",
    "            cluster_sum += (model.state_dict()[layer] [idx_dict[layer] == m+1]).sum() * float(gmp.scale[int(l/2)].exp())\n",
    "            #print (layer, float(mean), float(gmp.scale[int(l/2)].exp()), cluster_sum, cluster_elem)\n",
    "        new_means[m] = cluster_sum/cluster_elem\n",
    "    new_means\n",
    "\n",
    "    #-> Change model parameters according to decision boundaries\n",
    "    for l, layer in enumerate(model.state_dict()):\n",
    "        #-> Draw decision boundaries using means\n",
    "        nml = sorted(list(new_means / gmp.scale[int(l/2)].exp().data) + [0])\n",
    "        partitions = [(nml[i] + nml[i+1]) * 0.5 for i in range(len(nml)-1)]\n",
    "        value_dict[layer][model.state_dict()[layer] <= partitions[0]] = float(nml[0])\n",
    "        for p, partition in enumerate(partitions):\n",
    "            #print(value_dict[layer][value_dict[layer] < partition].numel())\n",
    "            #print (layer, partition, float(nml[p]), (model.state_dict()[layer] <= partition).sum())\n",
    "            value_dict[layer][model.state_dict()[layer] > partition] = float(nml[p+1])\n",
    "\n",
    "    gmp.means = new_means\n",
    "    model.load_state_dict(value_dict)\n",
    "    test_acc = test_accuracy(test_data_full, test_labels_full, model)\n",
    "    print('Repruned - Epoch: {}. Test Accuracy: {:.2f}'.format(1, test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3678847849369049"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18793274462223053"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.state_dict()['fc2.weight'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18788480758666992"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_dict['fc2.weight'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Softmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "#execution example: python retrain.py --model SWSModel --alpha 2500 --beta 10 --tau 1e-6 --mixtures 8 --temp 10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.nn.modules import Module\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import model_archs\n",
    "from utils_plot import show_sws_weights, show_weights, print_dims, prune_plot, draw_sws_graphs, joint_plot\n",
    "from utils_model import test_accuracy, train_epoch, retrain_sws_epoch, model_prune, get_weight_penalty\n",
    "from utils_misc import trueAfterN, logsumexp, model_load_dir\n",
    "from utils_sws import GaussianMixturePrior, special_flatten, KL, compute_responsibilies, merger, sws_prune\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "alpha = 25000\n",
    "beta = 10\n",
    "tau = 1e-5\n",
    "mixtures = 16\n",
    "model_name = \"SWSModel\"\n",
    "temp = 8\n",
    "\n",
    "#Data\n",
    "from mnist_loader import search_train_data, search_retrain_data, search_validation_data, train_data, test_data, batch_size\n",
    "model_dir = \"../models/\"\n",
    "training_epochs = 100\n",
    "retraining_epochs = 50\n",
    "model_file = 'mnist_{}_{}'.format(model_name, training_epochs)\n",
    "model = torch.load(model_dir + 'mnist_{}_{}_full.m'.format(model_name, 100)).cuda()\n",
    "\n",
    "trim_l = 40000\n",
    "\n",
    "if temp == 0:\n",
    "    temp = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    trimmed_data = train_dataset.train_data[trim_l:]\n",
    "    trimmed_labels = train_dataset.train_labels[trim_l:]\n",
    "    trimmed_dataset = (trimmed_data, trimmed_labels)\n",
    "    loader = torch.utils.data.DataLoader(dataset=trimmed_dataset, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    temp = float(temp)\n",
    "    criterion = nn.MSELoss()\n",
    "    output = torch.load(\"{}{}_full_targets/{}.out.m\".format(model_dir, model_file, \"fc2\"))\n",
    "    output = (nn.Softmax()(output/temp)).data#dim=1\n",
    "    \n",
    "    train_data_full = train_data(fetch = \"data\").cuda()\n",
    "    dataset = torch.utils.data.TensorDataset(train_data_full[trim_l:], output[trim_l:])\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Softmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-component Mean: 2500.0 Variance: 1250.0\n",
      "Non-zero component Mean: 2500.0 Variance: 250.0\n",
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "sizes do not match at /opt/conda/conda-bld/pytorch_1501969512886/work/pytorch-0.1.12/torch/lib/THC/generated/../generic/THCTensorMathPointwise.cu:344",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-134b75739a3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrain_sws_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmp_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrueAfterN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/okz21/NNC/NN_compression/src/utils_model.py\u001b[0m in \u001b[0;36mretrain_sws_epoch\u001b[0;34m(model, gmp, optimizer, criterion, train_loader, tau)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m#print (gmp.call())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m# Getting gradients w.r.t. parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mgmp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;31m#print( float(loss), float(gmp_loss) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/okz21/NNC/NN_compression/src/utils_sws.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, mask)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mweight_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixing_proportions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mweight_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixing_proportions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__div__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__div__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m     \u001b[0m__truediv__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__div__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mdiv\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/okz21/.conda/envs/nncpy3/lib/python3.5/site-packages/torch/autograd/_functions/basic_ops.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: sizes do not match at /opt/conda/conda-bld/pytorch_1501969512886/work/pytorch-0.1.12/torch/lib/THC/generated/../generic/THCTensorMathPointwise.cu:344"
     ]
    }
   ],
   "source": [
    "#exp_name = \"m{}_a{}_b{}_r{}_t{}_kdT{}\".format(model.name, alpha, beta, retraining_epochs, tau, temp)\n",
    "model_s = torch.load(model_dir + 'mnist_{}_{}_full.m'.format(model_name, 100)).cuda()\n",
    "gmp_s = GaussianMixturePrior(mixtures, [x for x in model_s.parameters()], 0.99, ab = (alpha, beta), scaling = True)\n",
    "\n",
    "sws_param1_s = [gmp_s.means]\n",
    "sws_param2_s = [gmp_s.gammas, gmp_s.rhos]\n",
    "\n",
    "opt = torch.optim.Adam([\n",
    "        {'params': model_s.parameters(), 'lr': 1e-4},\n",
    "        {'params': [gmp_s.means], 'lr': 1e-4},\n",
    "        {'params': [gmp_s.gammas, gmp_s.rhos], 'lr': 3e-3}])#log precisions and mixing proportions\n",
    "\n",
    "for epoch in range(retraining_epochs):\n",
    "    print(\"Epoch: {}\".format(epoch+1))\n",
    "    model_s, loss = retrain_sws_epoch(model_s, gmp_s, opt, criterion, loader, tau/10)\n",
    "    \n",
    "    if (trueAfterN(epoch, 5)):\n",
    "        test_acc = test_accuracy(test_data_full, test_labels_full, model_s)\n",
    "        show_sws_weights(model = model_s, means = list(gmp_s.means.data.clone().cpu()), precisions = list(gmp_s.gammas.data.clone().cpu()), epoch = epoch)\n",
    "        print('Epoch: {}. Test Accuracy: {:.2f}'.format(epoch+1, test_acc[0]))\n",
    "\n",
    "#torch.save(model, model_dir + model_file + '/mnist_model_retrain_{}.m'.format(exp_name))\n",
    "#with open(model_dir + model_file + '/mnist_model_retrain_{}_gmp.p'.format(exp_name),'wb') as f:\n",
    "#    pickle.dump(gmp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight', \n",
       "              (0 ,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.2912  0.0000  0.0000  0.0000  0.2912\n",
       "                0.0000  0.0000  0.2912  0.0000  0.5464\n",
       "                0.0000  0.2912  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000 -0.4062 -0.6069 -0.4062\n",
       "              \n",
       "              (1 ,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.4182  0.5464  0.0000 -0.4062\n",
       "                0.0000  0.4182  0.2912  0.0000 -0.4062\n",
       "                0.0000  0.0000  0.2912  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.2912  0.2912\n",
       "              \n",
       "              (2 ,0 ,.,.) = \n",
       "                0.0000  0.0000 -0.2955 -0.0315 -0.2955\n",
       "                0.0000  0.0000  0.0000  0.0000 -0.2955\n",
       "               -0.4062  0.0000  0.5464  0.5464  0.0000\n",
       "                0.0000  0.5464  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.4182  0.4182  0.0000\n",
       "              \n",
       "              (3 ,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.2912  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "              \n",
       "              (4 ,0 ,.,.) = \n",
       "                0.0000 -0.4062  0.0000 -0.4062  0.0000\n",
       "               -0.4062 -0.4062  0.0000  0.4182  0.0000\n",
       "               -0.4062  0.0000  0.2912  0.4182  0.0000\n",
       "                0.0000  0.2912  0.5464  0.4182  0.0000\n",
       "                0.0000  0.0000  0.4182  0.0000  0.0000\n",
       "              \n",
       "              (5 ,0 ,.,.) = \n",
       "                0.2912  0.0000 -0.2955 -0.4062  0.2912\n",
       "                0.0000 -0.2955 -0.4062 -0.4062 -0.4062\n",
       "                0.4182  0.0000  0.0000 -0.2955 -0.4062\n",
       "                0.5464  0.5464  0.2912  0.0000  0.0000\n",
       "                0.4182  0.5464  0.4182  0.2912  0.2912\n",
       "              \n",
       "              (6 ,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "              \n",
       "              (7 ,0 ,.,.) = \n",
       "                0.5464  0.4182  0.4182  0.2912  0.4182\n",
       "                0.5464  0.0000  0.5464  0.0000  0.0000\n",
       "                0.0000  0.0000 -0.2955 -0.2955 -0.2955\n",
       "                0.0000 -0.4062 -0.6069 -0.6069 -0.4062\n",
       "               -0.4062 -0.4062 -0.4062  0.0000  0.0000\n",
       "              \n",
       "              (8 ,0 ,.,.) = \n",
       "                0.0000  0.2912 -0.4062 -0.4062 -0.4062\n",
       "                0.2912  0.2912  0.2912 -0.4062 -0.4062\n",
       "                0.2912  0.5464  0.5464  0.0000 -0.2955\n",
       "                0.0000  0.4182  0.0000  0.4182  0.0000\n",
       "                0.0000  0.2912  0.0000  0.0000  0.0000\n",
       "              \n",
       "              (9 ,0 ,.,.) = \n",
       "                0.2912  0.2912  0.2912  0.5464  0.2912\n",
       "                0.1616  0.0000  0.0000  0.0000  0.0000\n",
       "               -0.4062 -0.6069  0.0000  0.0000 -0.6069\n",
       "                0.0000  0.0000 -0.2955 -0.2955  0.0000\n",
       "                0.4182  0.2912  0.5464  0.5464  0.4182\n",
       "              \n",
       "              (10,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000 -0.4062  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.2912  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.2912\n",
       "               -0.4062  0.0000 -0.4062  0.0000  0.0000\n",
       "              \n",
       "              (11,0 ,.,.) = \n",
       "                0.0000  0.2912  0.4182  0.4182  0.4182\n",
       "                0.0000  0.4182  0.2912  0.4182  0.5464\n",
       "               -0.2955  0.0000  0.0000  0.0000  0.2912\n",
       "               -0.4062 -0.4062  0.0000 -0.4062 -0.4062\n",
       "                0.0000 -0.4062 -0.4062 -0.4062 -0.4062\n",
       "              \n",
       "              (12,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.2912  0.0000\n",
       "                0.0000  0.4182  0.0000  0.2912  0.0000\n",
       "                0.4182  0.4182  0.0000 -0.4062  0.0000\n",
       "                0.0000  0.0000 -0.4062  0.0000  0.0000\n",
       "                0.0000 -0.2955  0.0000 -0.4062  0.0000\n",
       "              \n",
       "              (13,0 ,.,.) = \n",
       "                0.0000  0.2912  0.0000 -0.4062 -0.4062\n",
       "                0.0000  0.2912  0.4182  0.0000  0.0000\n",
       "                0.0000  0.2912  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "              \n",
       "              (14,0 ,.,.) = \n",
       "               -0.4062  0.0000  0.4182  0.4182  0.0000\n",
       "               -0.2955 -0.4062  0.2912  0.5464  0.4182\n",
       "               -0.4062  0.0000  0.0000  0.2912  0.4182\n",
       "               -0.6069  0.0000  0.0000  0.4182  0.4182\n",
       "               -0.4062  0.0000  0.0000  0.4182  0.0000\n",
       "              \n",
       "              (15,0 ,.,.) = \n",
       "                0.0000 -0.4062 -0.4062  0.0000  0.0000\n",
       "                0.0000 -0.6069 -0.6069  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.2912  0.4182  0.2912  0.0000\n",
       "                0.5464  0.5464  0.4182  0.4182 -0.2955\n",
       "              \n",
       "              (16,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.0000 -0.2955\n",
       "                0.0000  0.2912  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.2912  0.2912\n",
       "               -0.4062  0.0000 -0.4062  0.0000  0.2912\n",
       "                0.0000  0.0000 -0.4062  0.2912  0.0000\n",
       "              \n",
       "              (17,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.0000 -0.4062\n",
       "                0.2912  0.0000  0.2912  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000 -0.4062\n",
       "                0.0000  0.0000  0.0000  0.2912  0.0000\n",
       "                0.0000  0.0000  0.0000  0.2912  0.0000\n",
       "              \n",
       "              (18,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.2912  0.5464  0.5464  0.4182\n",
       "                0.0000  0.0000  0.4182  0.5464  0.4182\n",
       "               -0.4062 -0.2955  0.0000  0.0000  0.0000\n",
       "               -0.4062  0.0000  0.0000  0.0000  0.0000\n",
       "              \n",
       "              (19,0 ,.,.) = \n",
       "               -0.1714  0.2912  0.2912 -0.0315  0.0000\n",
       "               -0.2955  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.4182  0.4182  0.0000  0.0000\n",
       "                0.4182  0.5464  0.4182  0.4182 -0.2955\n",
       "                0.0000  0.0000  0.5464  0.0000 -0.2955\n",
       "              \n",
       "              (20,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.0000  0.2912\n",
       "                0.0000  0.2912  0.5464  0.4182  0.0000\n",
       "                0.5464  0.4182  0.5464 -0.0315  0.0000\n",
       "                0.5464  0.5464 -0.2955 -0.4062 -0.6069\n",
       "                0.0000  0.0000 -0.4062 -0.6069 -0.4062\n",
       "              \n",
       "              (21,0 ,.,.) = \n",
       "                0.0000  0.0000 -0.4062  0.0000  0.0000\n",
       "                0.0000  0.2912  0.2912  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "               -0.4062  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "              \n",
       "              (22,0 ,.,.) = \n",
       "                0.0000  0.0000 -0.4062  0.0000  0.0000\n",
       "                0.0000 -0.6069 -0.6069 -0.4062  0.2912\n",
       "                0.0000 -0.4062 -0.2955  0.4182  0.5464\n",
       "                0.0000  0.0000  0.5464  0.4182  0.0000\n",
       "                0.2912  0.2912  0.4182  0.4182  0.4182\n",
       "              \n",
       "              (23,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "              \n",
       "              (24,0 ,.,.) = \n",
       "               -0.4062 -0.4062 -0.4062 -0.4062  0.0000\n",
       "                0.0000  0.2912  0.0000  0.4182  0.4182\n",
       "                0.0000  0.2912  0.5464  0.4182 -0.2955\n",
       "                0.0000  0.2912  0.0000  0.0000  0.0000\n",
       "               -0.4062  0.0000 -0.2955  0.0000  0.4182\n",
       "              [torch.DoubleTensor of size 25x1x5x5]), ('conv1.bias', \n",
       "               0.0000\n",
       "              -0.2955\n",
       "               0.1616\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.4182\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.2912\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.2912\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.2912\n",
       "               0.0000\n",
       "               0.1616\n",
       "              -0.4062\n",
       "               0.0000\n",
       "              [torch.DoubleTensor of size 25]), ('conv2.weight', \n",
       "              (0 ,0 ,.,.) = \n",
       "               -0.2955  0.0000  0.0000\n",
       "                0.0000 -0.1714  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (0 ,1 ,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.2912\n",
       "               -0.2955  0.0000  0.0000\n",
       "              \n",
       "              (0 ,2 ,.,.) = \n",
       "               -0.1714  0.0000  0.4182\n",
       "               -0.1714  0.2912  0.2912\n",
       "               -0.1714  0.0000  0.2912\n",
       "                 ...\n",
       "              \n",
       "              (0 ,22,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000 -0.1714\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (0 ,23,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (0 ,24,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                   ⋮ \n",
       "              \n",
       "              (1 ,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (1 ,1 ,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "               -0.6069 -0.2955  0.0000\n",
       "                0.0000  0.2912  0.0000\n",
       "              \n",
       "              (1 ,2 ,.,.) = \n",
       "               -0.6069 -0.2955  0.0000\n",
       "                0.0000  0.1616  0.1616\n",
       "                0.0000  0.0000  0.0000\n",
       "                 ...\n",
       "              \n",
       "              (1 ,22,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.2912  0.1616  0.1616\n",
       "                0.1616  0.0000 -0.1714\n",
       "              \n",
       "              (1 ,23,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (1 ,24,.,.) = \n",
       "                0.0000  0.0000 -0.2955\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.1616  0.0000\n",
       "                   ⋮ \n",
       "              \n",
       "              (2 ,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000 -0.1714  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (2 ,1 ,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (2 ,2 ,.,.) = \n",
       "               -0.2955 -0.2955  0.0000\n",
       "                0.0000  0.0000  0.1616\n",
       "                0.0000  0.1616  0.1616\n",
       "                 ...\n",
       "              \n",
       "              (2 ,22,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.1616\n",
       "                0.4182  0.1616  0.0000\n",
       "              \n",
       "              (2 ,23,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (2 ,24,.,.) = \n",
       "                0.0000 -0.2955  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.1616\n",
       "              ...   \n",
       "                   ⋮ \n",
       "              \n",
       "              (47,0 ,.,.) = \n",
       "                0.0000  0.0000  0.2912\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000 -0.1714\n",
       "              \n",
       "              (47,1 ,.,.) = \n",
       "                0.2912  0.0000 -0.1714\n",
       "                0.0000  0.0000  0.0000\n",
       "               -0.1714  0.0000  0.0000\n",
       "              \n",
       "              (47,2 ,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "               -0.6069 -0.6069 -0.1714\n",
       "                0.0000  0.0000  0.0000\n",
       "                 ...\n",
       "              \n",
       "              (47,22,.,.) = \n",
       "                0.0000  0.2912  0.2912\n",
       "               -0.1714 -0.1714 -0.1714\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (47,23,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (47,24,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000 -0.6069\n",
       "                   ⋮ \n",
       "              \n",
       "              (48,0 ,.,.) = \n",
       "               -0.2955  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (48,1 ,.,.) = \n",
       "                0.1616  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (48,2 ,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.2912  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                 ...\n",
       "              \n",
       "              (48,22,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "               -0.2955 -0.2955  0.0000\n",
       "              \n",
       "              (48,23,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (48,24,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                   ⋮ \n",
       "              \n",
       "              (49,0 ,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (49,1 ,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "               -0.1714  0.0000  0.0000\n",
       "              \n",
       "              (49,2 ,.,.) = \n",
       "                0.0000  0.1616  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                 ...\n",
       "              \n",
       "              (49,22,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.1616  0.0000  0.0000\n",
       "              \n",
       "              (49,23,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              \n",
       "              (49,24,.,.) = \n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "                0.0000  0.0000  0.0000\n",
       "              [torch.DoubleTensor of size 50x25x3x3]), ('conv2.bias', \n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "              [torch.DoubleTensor of size 50]), ('fc1.weight', \n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "                        ...             ⋱             ...          \n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "              [torch.DoubleTensor of size 500x1250]), ('fc1.bias', \n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "              [torch.DoubleTensor of size 500]), ('fc2.weight', \n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "               0.0000  0.0000  0.0000  ...   0.0000 -0.1714  0.0000\n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "                        ...             ⋱             ...          \n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "               0.0000  0.0000 -0.1714  ...   0.0000  0.0000  0.0000\n",
       "               0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "              [torch.DoubleTensor of size 10x500]), ('fc2.bias', \n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.0000\n",
       "               0.1616\n",
       "               0.0000\n",
       "              [torch.DoubleTensor of size 10])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sws_prune(model_s, gmp_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
